{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %pip install tiktoken langchain langchain_experimental langchain_openai"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1714554882918
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /home/azureuser/cloudfiles/code/blobfuse/blobfuse_raadsinformatie.sh"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684607464
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select where to run notebook: \"azure\" or \"local\"\n",
        "# my_run = \"azure\"\n",
        "my_run = \"local\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684608464
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import my_secrets as sc\n",
        "import settings as st\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    import config_azure as cf\n",
        "elif my_run == \"local\":\n",
        "    import config as cf"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684609386
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if my_run == \"azure\":\n",
        "    if not os.path.exists(cf.HUGGING_CACHE):\n",
        "        os.mkdir(cf.HUGGING_CACHE)\n",
        "\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = cf.HUGGING_CACHE"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684609964
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "comparison_folder = f\"{cf.raadsinformatie_out_folder}/comparison/\"\n",
        "Path(comparison_folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "chunking_comparison_file = Path(comparison_folder, \"chunking_comparison.csv\")\n",
        "chunking_comparison_file_xlsx = Path(comparison_folder, \"chunking_comparison.xlsx\")\n",
        "\n",
        "chunking_folder = f\"{cf.raadsinformatie_out_folder}/comparison/chunking/\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684615974
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "woo_dirs = \\\n",
        "        [f\"{cf.woo_sources['openamsterdam']}/{folder}\" for folder in os.listdir(cf.woo_sources['openamsterdam'])] + \\\n",
        "        [f\"{cf.woo_sources['raadsinformatie']}/{folder}\" for folder in os.listdir(cf.woo_sources['raadsinformatie'])] + \\\n",
        "        [f\"{cf.woo_sources['amsterdam.nl']}/{folder}\" for folder in os.listdir(cf.woo_sources['amsterdam.nl'])]\n",
        "\n",
        "woo_files = sum([glob.glob(f\"{folder}/*.ocr\") for folder in woo_dirs], [])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684620523
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(woo_files)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684621493
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "\n",
        "if chunking_comparison_file.exists():\n",
        "# if False:\n",
        "    df = pd.read_csv(chunking_comparison_file)\n",
        "else:\n",
        "    documents = [open(woo_file, \"r\").read() for woo_file in woo_files]\n",
        "    df = pd.DataFrame.from_dict({\n",
        "        \"path\": woo_files,\n",
        "        # \"path_rel\": map(lambda x: x.replace(f\"{cf.woo_sources['openamsterdam']}\", \"\"), woo_files),\n",
        "        \"short_path\": map(lambda x: x.removeprefix(cf.raadsinformatie_in_folder), woo_files),\n",
        "        \"file_name\": map(lambda x: x.split(\"/\")[-1], woo_files),\n",
        "        \"doc\": documents\n",
        "    })\n",
        "\n",
        "for column in df.columns:\n",
        "    if column.startswith(\"chunks\"):\n",
        "        df[column] = df[column].apply(literal_eval)\n",
        "\n",
        "# # Temp fix of old bug\n",
        "# df[\"short_path\"] = df[\"path\"].map(lambda x: x.removeprefix(cf.raadsinformatie_in_folder))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684657510
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import tiktoken\n",
        "\n",
        "tiktoken_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "df[\"len\"] = df.doc.str.len()\n",
        "df[\"len_char\"] = df.doc.str.len()\n",
        "df[\"len_word_token\"] = df.doc.map(lambda x: len(word_tokenize(x)))\n",
        "# Rule of thumb (seems to be majorly underestimating)\n",
        "df[\"len_token_appr\"] = df[\"len_char\"] // 4\n",
        "# https://stackoverflow.com/questions/75804599/openai-api-how-do-i-count-tokens-before-i-send-an-api-request\n",
        "df[\"len_token_tiktoken\"] = df.doc.map(lambda x: len(tiktoken_encoding.encode(x)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684677385
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684680116
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunk_overlap = 25\n",
        "\n",
        "for chunk_size in [256, 512, 8191]:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap,\n",
        "        # separator=\"\\n\\n\", \n",
        "        # is_separator_regex=False,\n",
        "        # strip_whitespace=False\n",
        "    )\n",
        "    # df[f\"chunks_basic-{chunk_size}\"] = df.doc.map(lambda x: text_splitter.create_documents([x]))\n",
        "    df[f\"chunks_basic-{chunk_size}\"] = df.doc.map(lambda x: text_splitter.split_text(x))\n",
        "    df[f\"len_chunks_basic-{chunk_size}\"] = df[f\"chunks_basic-{chunk_size}\"].map(lambda x: len(x))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684684398
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684689598
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "\n",
        "text_splitter_spacy = SpacyTextSplitter(pipeline=\"nl_core_news_sm\")\n",
        "df[\"chunks_spacy\"] = df.doc.map(lambda x: text_splitter_spacy.split_text(x))\n",
        "df[\"len_chunks_spacy\"] = df.chunks_spacy.map(lambda x: len(x))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684767411
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from src.llms.model_config import get_embed_model\n",
        "\n",
        "hf_models = {\n",
        "    \"bert\": \"jegormeister/bert-base-dutch-cased-snli\",\n",
        "    \"robbert\": \"NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\",\n",
        "    # \"cohere\": \"Cohere/Cohere-embed-multilingual-v3.0\",\n",
        "    \"me5\": \"intfloat/multilingual-e5-large-instruct\",\n",
        "}\n",
        "\n",
        "for model, model_id in hf_models.items():\n",
        "    print(model_id)\n",
        "    # print(text_splitter_hugging_face._chunk_overlap)\n",
        "    # print(text_splitter_hugging_face._chunk_size)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    text_splitter_hugging_face = TokenTextSplitter.from_huggingface_tokenizer(tokenizer)\n",
        "    df[f\"chunks_{model}\"] = df.doc.map(lambda x: text_splitter_hugging_face.split_text(x))\n",
        "    df[f\"len_chunks_{model}\"] = df[f\"chunks_{model}\"].map(lambda x: len(x))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684778355
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from src.llms.model_config import get_embed_tokenizer\n",
        "\n",
        "for model in [\"bert\", \"robbert\", \"me5\", \"me5-instruct\", \"cohere\"]:\n",
        "    tokenizer, chunk_size = get_embed_tokenizer(model)\n",
        "    # print(tokenizer)\n",
        "    text_splitter_hugging_face = TokenTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=chunk_size, chunk_overlap=0)\n",
        "    df[f\"chunks_{model}\"] = df.doc.map(lambda x: text_splitter_hugging_face.split_text(x))\n",
        "    df[f\"len_chunks_{model}\"] = df[f\"chunks_{model}\"].map(lambda x: len(x))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684794642
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text_splitter_hugging_face._chunk_overlap\n",
        "# text_splitter_hugging_face._chunk_size"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684794856
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721684795046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dump all so far"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(chunking_comparison_file, index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714728976020
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install xlsxwriter"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(chunking_comparison_file_xlsx, engine='xlsxwriter')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714780864706
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710403796032
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.llms.model_config import get_embed_model_info, KNOWN_EMBED_MODELS\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = sc.AZURE_OPENAI_API_KEY\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = st.AZURE_OPENAI_ENDPOINT\n",
        "\n",
        "def get_embedding_model(model_name):\n",
        "    if model_name == \"ada\":\n",
        "        return AzureOpenAIEmbeddings(\n",
        "            model=\"text-embedding-ada-002\",\n",
        "            # deployment_name=\"text-embedding-ada-002\",\n",
        "            api_key=sc.AZURE_OPENAI_API_KEY,\n",
        "            azure_endpoint=st.AZURE_OPENAI_ENDPOINT,\n",
        "            api_version=\"2023-05-15\",\n",
        "        )\n",
        "        \n",
        "    elif model_name in KNOWN_EMBED_MODELS:\n",
        "        model_id, chunk_size = get_embed_model_info(model_name)\n",
        "        return HuggingFaceEmbeddings(model_name=model_id)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model {model_name}. Known models: {KNOWN_EMBED_MODELS}\")\n",
        "\n",
        "# for model in list(hf_models.keys()) + [\"ada\"]:\n",
        "# for model in [\"bert\", \"robbert\", \"me5\", \"me5-instruct\", \"cohere\", \"ada\"]:\n",
        "for model in [\"bert\", \"robbert\", \"me5\"]:\n",
        "# for model in [\"me5-instruct\", \"cohere\"]:\n",
        "    try:\n",
        "        print(model)\n",
        "\n",
        "        # if f\"chunks_semantic_splitter_{model}\" not in df:\n",
        "        if True:\n",
        "            embed_model = get_embedding_model(model)\n",
        "            semantic_splitter = SemanticChunker(embed_model)\n",
        "            df[f\"chunks_semantic_splitter_{model}\"] = df.doc.progress_map(lambda x: semantic_splitter.split_text(x))\n",
        "            df[f\"len_chunks_semantic_splitter_{model}\"] = df[f\"chunks_semantic_splitter_{model}\"].progress_map(lambda x: len(x))\n",
        "            df.to_csv(chunking_comparison_file, index=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714746612250
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "me5-instruct -> 1h48m"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for ind, row in tqdm(df.iterrows()):\n",
        "    short_path = row[\"path\"].removeprefix(cf.raadsinformatie_in_folder)\n",
        "    output_folder = Path(chunking_folder + short_path)\n",
        "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
        "    for field, val in row.items():\n",
        "        if field.startswith(\"chunks\"):\n",
        "            with open(Path(output_folder, field), \"w\") as f:\n",
        "                f.write(f\"\\n{50*'='}\\n\".join(row[field]))\n",
        "        "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714746862836
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df[[\"short_path\", \"file_name\", \"doc\", \"chunks_basic-256\", \"chunks_bert\"]][df[\"short_path\"].str.contains(\"manual\")]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714556374781
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"max_colwidth\", 50)\n",
        "\n",
        "df.describe().applymap(lambda x: f\"{x:0.1f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1715640870982
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df[df[\"len_chunks_me5\"] > 1] \n",
        "# df[\"len_word_token\"] / df[\"len_chunks_semantic_splitter_me5\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714713178816
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "woo-py39",
      "language": "python",
      "display_name": "woo-py39"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "woo-py39"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}